{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio del algoritmo de Clasificación Naïve Bayes \n",
    "\n",
    "Este es uno de los algoritmos mas simples y poderosos para clasificación de datos. Trabaja sumamente bien con grandes cantidades de datos, y su velocidad y versatilidad lo hacen uno de los favoritos para un primer acercamiento a modelos de clasificación. Hay muchas variantes de este algoritmo, sin embargo, en este lab., desarrollaremos el clásico. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías que importaremos:\n",
    "\n",
    "# Librería de aprendizaje automático.\n",
    "import sklearn \n",
    "\n",
    "# Librería de graficación\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# En esta ocación importaremos uno de los datasets que vienen con la librería.\n",
    "from sklearn import datasets \n",
    "\n",
    "# Hemos elegido un conjunto de datos relacionado con la industria del vino (de nuevo).\n",
    "vinos = datasets.load_wine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos corresponden al análisis realizado por un laboratorio químico a un conjunto de  178 muestras de vinos. Estos corresponden a tres grandes casas productoras de vino en la misma región de Italia. El reto de este laboratorio es predecir de cual de los cultivadores de vino proviene la muestra analizada. Para cada muestra se caracterizan 13 de sus constituyentes. Por lo que es un problema alto-dimensional, en donde graficar es una tarea ligeramente compleja (aunque no imposible). \n",
    "\n",
    "Fuente: Forina, M. et al, PARVUS - An Extendible Package for Data Exploration, Classification and Correlation. Institute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno, 16147 Genoa, Italy. \n",
    "\n",
    "Atributos reportados en las muestras:\n",
    "\n",
    "1) Alcohol. \n",
    "\n",
    "2) Ácido málico. \n",
    "\n",
    "3) Ceniza.  \n",
    "\n",
    "4) Alcalinidad de la ceniza.\n",
    "\n",
    "5) Magnesio. \n",
    "\n",
    "6) Fenoles totales.\n",
    "\n",
    "7) Flavanoides. \n",
    "\n",
    "8) Fenoles no flavonoides.\n",
    "\n",
    "9) Proantocianidina. \n",
    "\n",
    "10) Intensidad de Color.\n",
    "\n",
    "11) Hue. \n",
    "\n",
    "12) OD280/OD315 de vinos diluidos.\n",
    "\n",
    "13) Prolina.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propiedades: ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n",
      "En total: 13\n"
     ]
    }
   ],
   "source": [
    "# Veamos las propiedades de los vinos, disponibles en este conjunto de datos. \n",
    "print('Propiedades:', vinos.feature_names)\n",
    "print('En total:', len(vinos.feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiquetas:  ['class_0' 'class_1' 'class_2']\n"
     ]
    }
   ],
   "source": [
    "# Y la variable objetivo o etiqueta que las acompañan\n",
    "print(\"Etiquetas: \", vinos.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que los vinos son clasificados en tres clases. Finalmente, veamos cuantos registros tiene nuestro conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n"
     ]
    }
   ],
   "source": [
    "print(vinos.data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos hacer una revisión de las primeras 3 filas de nuestro conjunto de datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00\n",
      "  2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03]\n",
      " [1.320e+01 1.780e+00 2.140e+00 1.120e+01 1.000e+02 2.650e+00 2.760e+00\n",
      "  2.600e-01 1.280e+00 4.380e+00 1.050e+00 3.400e+00 1.050e+03]\n",
      " [1.316e+01 2.360e+00 2.670e+00 1.860e+01 1.010e+02 2.800e+00 3.240e+00\n",
      "  3.000e-01 2.810e+00 5.680e+00 1.030e+00 3.170e+00 1.185e+03]]\n"
     ]
    }
   ],
   "source": [
    "print(vinos.data[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# También podemos ver la etiqueta de la clase codificada para cada vino:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(vinos.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedemos entonces a crear y entrenar nuestro modelo de Naïve Bayes. Empezamos dividiendo nuestro conjunto de datos en datos de entrenamiento y testing/prueba, usando la función de sklearn train_test_split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y creamos las variables que nos permitirán separar nuestro conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(vinos.data, vinos.target, test_size=0.2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hemos tomado 80% para entrenamiento y 20% para el proceso de validación de nuestro modelo. En lo siguiente supondremos que la verosimilitud de las propiedades siguen una distribución Gaussiana. En general, esta hipotesis es bastante buena, aunque dependiendo del tipo de datos que tengamos, puede que convenga, i.e., pensar en una distribución binomial (para clasificación binaria), o la variante de Naive Bayes complementario (basado en distribuciones multinomiales) para variables no-balanceadas. En este caso utilizaremos la elección mas común, i.e., el modelo de Bayes Gaussiano:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos la función que nos permitirá crear nuestros clasificadores Gaussianos:\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# y creamos nuestro clasificador \n",
    "gnb = GaussianNB()\n",
    "\n",
    "# ¿Muy fácil no? ... esto es la denominada democratización de la inteligencia artíficial.\n",
    "\n",
    "# Entrenamos luego nuestro modelo usando los datos de entrenamiento, y usamos el método fit. \n",
    "gnb.fit(X_train, Y_train)\n",
    "\n",
    "# y finalmente usamos el método predict para predecir sobre los datos de prueba.\n",
    "Y_pred = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9444444444444444%\n"
     ]
    }
   ],
   "source": [
    "# En este caso mediremos el rendimiento de este modelo usando accuracy \n",
    "from sklearn import metrics\n",
    "\n",
    "# La calculamos y mostramos\n",
    "print(\"Accuracy:\"+str(metrics.accuracy_score(Y_test, Y_pred))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el vino con los atributos químicos: \n",
      "\n",
      " [1.237e+01 1.170e+00 1.920e+00 1.960e+01 7.800e+01 2.110e+00 2.000e+00\n",
      " 2.700e-01 1.040e+00 4.680e+00 1.120e+00 3.480e+00 5.100e+02]\n",
      "\n",
      "\n",
      "Naïve Bayes lo predice en la clase  1\n",
      "y en efecto pertenece a la clase  1\n"
     ]
    }
   ],
   "source": [
    "print('el vino con los atributos químicos: \\n\\n', X_test[1])\n",
    "print('\\n')\n",
    "print('Naïve Bayes lo predice en la clase ', Y_pred[1])\n",
    "print('y en efecto pertenece a la clase ', Y_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Contenplad el poder de la Inteligencia Artificial'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
